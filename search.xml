<?xml version="1.0" encoding="utf-8"?>
<search> 
  
  
    
    <entry>
      <title>Spark中宽依赖和窄依赖</title>
      <link href="/2020/09/21/spark-zhong-kuan-yi-lai-he-zhai-yi-lai/"/>
      <url>/2020/09/21/spark-zhong-kuan-yi-lai-he-zhai-yi-lai/</url>
      
        <content type="html"><![CDATA[<h2 id="窄依赖"><a href="#窄依赖" class="headerlink" title="窄依赖"></a>窄依赖</h2><pre class=" language-scala"><code class="language-scala">df<span class="token punctuation">.</span>filter<span class="token punctuation">(</span>$<span class="token string">"col"</span> <span class="token operator">=</span><span class="token operator">!=</span> lit<span class="token punctuation">(</span><span class="token string">"value"</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token comment" spellcheck="true">// map union 等算子</span>filter Rdd 分区 map 模拟<span class="token punctuation">(</span>k1<span class="token punctuation">,</span>v1<span class="token punctuation">)</span> <span class="token keyword">=></span> <span class="token punctuation">(</span>k1<span class="token punctuation">,</span>v1<span class="token punctuation">)</span><span class="token punctuation">(</span>k2<span class="token punctuation">,</span>v2<span class="token punctuation">)</span> <span class="token keyword">=></span> <span class="token punctuation">(</span>k2<span class="token punctuation">,</span>v2<span class="token punctuation">)</span><span class="token punctuation">(</span>k3<span class="token punctuation">,</span>v3<span class="token punctuation">)</span> <span class="token keyword">=></span> <span class="token punctuation">(</span>k3<span class="token punctuation">,</span>v3<span class="token punctuation">)</span></code></pre><p>map和filter算子中，对于父RDD来说，一个分区内的数据，<strong>有且仅有一个</strong>子RDD的分区来消费该数据,<br>故判断窄依赖的依据: 父类分区内的数据，会被子类RDD中的指定的唯一一个分区所消费</p><h2 id="宽依赖"><a href="#宽依赖" class="headerlink" title="宽依赖"></a>宽依赖</h2><p>只要是shuffle,一定会产生宽依赖;<br>宽依赖，指的是父类一个分区内的数据，会被子RDD内的<strong>多个分区</strong>消费，需要自行判断分区，来实现数据发送的效果.</p><h2 id="思考"><a href="#思考" class="headerlink" title="思考"></a>思考</h2><ol><li><p>每个分区内的数据，是否能够指定自己在子类RDD中的分区？<br>答:如果不能，那就是宽依赖：如果父RDD和子RDD分区数目一致，那基本就是窄依赖了</p></li><li><p>为什么hadoop没有窄依赖?<br>答:map于reduce之间都需要shuffle</p></li><li><p>判断宽窄依赖的根据到底是什么?<br>答: RDD产生的数据流向;父RDD中的一个Partition对子RDD进行数据分发,则为宽依赖,否则称之为窄依赖。即使父RDD的分区只有一条数据，它也走了数据分发的逻辑，只不过按照流程走了一遍，但这无法改变它开启了数据分发的事实。</p></li></ol>]]></content>
      
      
      
        <tags>
            
            <tag> Spark </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>关于HDFS的几个问题</title>
      <link href="/2020/09/21/guan-yu-hdfs-de-ji-ge-wen-ti/"/>
      <url>/2020/09/21/guan-yu-hdfs-de-ji-ge-wen-ti/</url>
      
        <content type="html"><![CDATA[<h2 id="1-关于向hdfs写数据时-假设复制因子为3-向3个节点写数据完成时-是同时发送ack-还是写完一个就发送"><a href="#1-关于向hdfs写数据时-假设复制因子为3-向3个节点写数据完成时-是同时发送ack-还是写完一个就发送" class="headerlink" title="1. 关于向hdfs写数据时,假设复制因子为3,向3个节点写数据完成时,是同时发送ack,还是写完一个就发送?"></a>1. 关于向hdfs写数据时,假设复制因子为3,向3个节点写数据完成时,是同时发送ack,还是写完一个就发送?</h2><p>答:每个DataNode写完一个块后，会返回确认信息。实际上，hdfs write的时候只需要把数据写到缓冲区就可以返回进行下一条写操作了，连写成功第一个节点都不必要，真正计算起来基本上就是 数据大小/网络带宽 + 3 * 网络传输时延了。</p><h2 id="2-为什么不同时向3个DataNode-发送数据"><a href="#2-为什么不同时向3个DataNode-发送数据" class="headerlink" title="2. 为什么不同时向3个DataNode 发送数据?"></a>2. 为什么不同时向3个DataNode 发送数据?</h2><p>答:网络带宽问题</p><h2 id="3-怎么确定数据节点的远近-怎么得到最近的节点了"><a href="#3-怎么确定数据节点的远近-怎么得到最近的节点了" class="headerlink" title="3. 怎么确定数据节点的远近?怎么得到最近的节点了?"></a>3. 怎么确定数据节点的远近?怎么得到最近的节点了?</h2><p>答:节点距离：两个节点到达最近的共同祖先的距离总和<br>在本地网络中，两个节点被称为”彼此近邻”,在海量数据处理中，其主要限制因素是节点之间数据的传输效率——带宽很稀缺。<br>网路拓扑: 根节点为 “<strong>/</strong>“,其子节点为交换机,再其子节点为机架,机架的子节点就是DataNode机器了,知道其信息,就可以算出节点间的距离了。</p>]]></content>
      
      
      <categories>
          
          <category> 大数据 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Hadoop </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Spark程序执行流程</title>
      <link href="/2020/09/21/spark-cheng-xu-zhi-xing-liu-cheng/"/>
      <url>/2020/09/21/spark-cheng-xu-zhi-xing-liu-cheng/</url>
      
        <content type="html"><![CDATA[<h2 id="执行流程"><a href="#执行流程" class="headerlink" title="执行流程"></a>执行流程</h2><ol><li>当 Driver 进程被启动之后,首先它将发送请求到Master节点上,进行Spark应用程序的注册</li><li>Master在接受到Spark应用程序的注册申请之后,会发送给Worker,让其进行资源的调度和分配.</li><li>Worker 在接受Master的请求之后,会为Spark应用程序启动Executor, 来分配资源</li><li>Executor启动分配资源好后,就会想Driver进行反注册,这是Driver已经知道哪些Executor为他服务了</li><li>当Driver得到注册了Executor之后,就可以开始正式执行spark应用程序了. 首先第一步,就是创建初始RDD,读取数据源,再执行之后的一系列算子. HDFS文件内容被读取到多个worker节点上,形成内存中的分布式数据集,也就是初始RDD</li><li>Driver就会根据 Job 任务任务中的算子形成对应的task,最后提交给 Executor, 来分配给task进行计算的线程</li><li>task就会去调用对应的任务数据来计算,并task会对调用过来的RDD的partition数据执行指定的算子操作,形成新的RDD的partition,这时一个大的循环就结束了</li><li>后续的RDD的partition数据又通过Driver形成新的一批task提交给Executor执行,循环这个操作,直到所有的算子结束</li></ol>]]></content>
      
      
      <categories>
          
          <category> 大数据 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Spark </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Hadoop的HA</title>
      <link href="/2020/09/21/hadoop-de-ha/"/>
      <url>/2020/09/21/hadoop-de-ha/</url>
      
        <content type="html"><![CDATA[<h2 id="HA介绍"><a href="#HA介绍" class="headerlink" title="HA介绍"></a>HA介绍</h2><ul><li><strong>Active NameNode</strong> 和 <strong>Standby NameNode</strong>：两台 NameNode 形成互备，一台处于 Active 状态，为主 NameNode，另外一台处于 Standby 状态，为备 NameNode，只有主 NameNode 才能对外提供读写服务；</li><li><strong>ZKFailoverController</strong>（主备切换控制器，FC）：ZKFailoverController 作为独立的进程运行，对 NameNode 的主备切换进行总体控制。ZKFailoverController 能及时检测到 NameNode 的健康状况，在主 NameNode 故障时借助 Zookeeper 实现自动的主备选举和切换（当然 NameNode 目前也支持不依赖于 Zookeeper 的手动主备切换）；<br>Zookeeper 集群：为主备切换控制器提供主备选举支持；</li><li><strong>共享存储系统</strong>：共享存储系统是实现 NameNode 的高可用最为关键的部分，共享存储系统保存了 NameNode 在运行过程中所产生的 HDFS 的元数据。主 NameNode 和备 NameNode 通过共享存储系统实现元数据同步。在进行主备切换的时候，新的主 NameNode 在确认元数据完全同步之后才能继续对外提供服务。</li><li><strong>DataNode 节点</strong>：因为主 NameNode 和备 NameNode 需要共享 HDFS 的数据块和 DataNode 之间的映射关系，为了使故障切换能够快速进行，DataNode 会同时向主 NameNode 和备 NameNode 上报数据块的位置信息。</li></ul>]]></content>
      
      
      <categories>
          
          <category> 大数据 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Hadoop </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>MapReduce简介</title>
      <link href="/2020/09/21/mapreduce-jian-jie/"/>
      <url>/2020/09/21/mapreduce-jian-jie/</url>
      
        <content type="html"><![CDATA[<h2 id="概述"><a href="#概述" class="headerlink" title="概述"></a>概述</h2><p>Hadoop MapReduce 是一个分布式计算框架，用于编写批处理应用程序。编写好的程序可以提交到 Hadoop 集群上用于并行处理大规模的数据集。<br>MapReduce 作业通过将输入的数据集拆分为独立的块，这些块由 map 以并行的方式处理，框架对 map 的输出进行排序，然后输入到 reduce 中。</p><h2 id="编程模型简述"><a href="#编程模型简述" class="headerlink" title="编程模型简述"></a>编程模型简述</h2><ul><li><strong>Map阶段</strong>: <ul><li><strong>input</strong>: 读取文件</li><li><strong>split</strong>: 将文件按照行进行拆分，每个输入分片（input split）针对一个map任务</li><li><strong>map</strong>: 就是程序员编写好的map函数了，因此map函数效率相对好控制，而且一般map操作都是本地化操作也就是在数据存储节点上进行</li><li><strong>shuffle</strong>: <ul><li><strong>partition</strong>: 需要计算每一个map的结果需要发到哪个reduce端,partition数等于reducer数.默认采用HashPartition</li><li><strong>spill</strong>: 此阶段分为sort和combine.首先分区过得数据会经过排序之后写入环形内存缓冲区.在达到阈值之后守护线程将数据溢出分区文件</li><li><strong>merge</strong>: spill结果会有很多个文件,但最终输出只有一个,故有一个merge操作会合并所有的本地文件,并且该文件会有一个对应的索引文件</li></ul></li></ul></li><li><strong>Reduce阶段</strong>: <ul><li><strong>copy</strong>: 拉取数据,reduce启动数据copy线程(默认5个),通过Http请求对应节点的map task输出文件,copy的数据也会先放到内部缓冲区.之后再溢写,类似map端操作</li><li><strong>marge</strong>: 合并多个copy的多个map端的数据.在一个reduce端先将多个map端的数据溢写到本地磁盘,之后再将多个文件合并成一个文件</li><li><strong>output</strong>: merge阶段最后会生成一个文件,将此文件转移到内存中,shuffle阶段结束</li><li><strong>reduce</strong>: 开始执行reduce任务,输出</li></ul></li></ul>]]></content>
      
      
      <categories>
          
          <category> 大数据 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Hadoop </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>HDFS分布式文件系统</title>
      <link href="/2020/09/21/hdfs-fen-bu-shi-wen-jian-xi-tong/"/>
      <url>/2020/09/21/hdfs-fen-bu-shi-wen-jian-xi-tong/</url>
      
        <content type="html"><![CDATA[<h2 id="介绍"><a href="#介绍" class="headerlink" title="介绍"></a>介绍</h2><p>HDFS （Hadoop Distributed File System）是 Hadoop 下的分布式文件系统，具有高容错、高吞吐量等特性，可以部署在低成本的硬件上。</p><h2 id="HDFS-架构设计及原理"><a href="#HDFS-架构设计及原理" class="headerlink" title="HDFS 架构设计及原理"></a>HDFS 架构设计及原理</h2><h3 id="HDFS-架构"><a href="#HDFS-架构" class="headerlink" title="HDFS 架构"></a>HDFS 架构</h3><p>HDFS 遵循主/从架构，由单个 NameNode(NN) 和多个 DataNode(DN) 组成：</p><ul><li>NameNode : 负责执行有关 文件系统命名空间 的操作，例如打开，关闭、重命名文件和目录等。它同时还负责集群元数据的存储，记录着文件中各个数据块的位置信息。</li><li>DataNode：负责提供来自文件系统客户端的读写请求，执行块的创建，删除等操作。<br>NameNode节点分为 Active NameNode 和 Standby NameNode(作为主备切换)</li></ul><h3 id="文件系统命名空间"><a href="#文件系统命名空间" class="headerlink" title="文件系统命名空间"></a>文件系统命名空间</h3><p>HDFS的<strong>文件系统命名空间</strong>的层次结构与大多数文件系统类似 (如 Linux)， 支持目录和文件的创建、移动、删除和重命名等操作，支持配置用户和访问权限，但不支持硬链接和软连接。NameNode 负责维护文件系统名称空间，记录对名称空间或其属性的任何更改。</p><h3 id="数据复制"><a href="#数据复制" class="headerlink" title="数据复制"></a>数据复制</h3><p>为了保证数据的可靠性和容错性,提供了数据复制的机制.HDFS将存储的文件分成一系列的块,每个块由多个副本来保证容错性,默认配置下,每块为128M,复制因子为3(复制3份)。</p><h3 id="数据复制的实现原理"><a href="#数据复制的实现原理" class="headerlink" title="数据复制的实现原理"></a>数据复制的实现原理</h3><p>HDFS 采用机架感知副本放置策略，对于常见情况，当复制因子为 3 时，HDFS 的放置策略是：<br>在写入程序位于<strong>datanode</strong>上时，就优先将写入文件的一个副本放置在该<strong>datanode</strong>上，否则放在随机<strong>datanode</strong>上。之后在另一个远程机架上的任意一个节点上放置另一个副本，并在该机架上的另一个节点上放置最后一个副本。此策略可以减少机架间的写入流量，从而提高写入性能。<br>如果复制因子大于 3，则随机确定第 4 个和之后副本的放置位置，同时保持每个机架的副本数量低于上限，上限值通常为 （复制系数 - 1）/ 机架数量 + 2，需要注意的是不允许同一个<strong>datanode</strong>上具有同一个块的多个副本。</p><h3 id="副本的选择机制"><a href="#副本的选择机制" class="headerlink" title="副本的选择机制"></a>副本的选择机制</h3><p>为了最大限度地减少带宽消耗和读取延迟，HDFS 在执行读取请求时，优先读取距离读取器<strong>最近</strong>的副本。如果在与读取器节点相同的机架上存在副本，则优先选择该副本。如果 HDFS 群集跨越多个数据中心，则优先选择<strong>本地数据中心</strong>上的副本。</p><h3 id="其他"><a href="#其他" class="headerlink" title="其他"></a>其他</h3><p>为了保证稳定性,每个DataNode会定期向NameNode发送心跳,如果超出了指定时间,则就会将这个DataNode标记为死亡状态,不可用了,那么就会导致数据也不可用了,NameNode就会跟踪这些数据块,在必要的时候进行重写复制。<br>数据完整性:如果数据遭到某些原因的损坏,而造成的读取错误,该怎么解决了?<br>    当客户端创建 HDFS 文件时,它会计算文件的每个块的 校验和,并将 校验和 存储在同一 HDFS 命名空间下的单独的隐藏文件中。当客户端检索文件内容时,它会验证从每个 DataNode 接收的数据是否与存储在关联校验和文件中的 校验和 匹配。如果匹配失败,则证明数据已经损坏,此时客户端会选择从其他 DataNode 获取该块的其他可用副本。<br>FsImage 和 EditLog:</p><ul><li>FsImage保存来最新的元数据检查点,包含来整个hdfs文件系统的所有目录和文件的信息。对于文件来说包括了数据块描述信息,修改时间,访问时间等,对于目录来说包括修改时间,访问权限控制信息（目录所属用户，所在组）等。Fimage就是在某一时刻,整个hdfs的快照,就是这个时刻的hdfs上所有的文件块和目录,分别的状态,位于哪些个datanode,各自的权限,各自的副本个数。</li><li>EditLog主要是在Namenode已经启动情况下对hdfs进行的各种更新操作进行记录，hdfs客户端执行所有的写操作都会被记录到editlog中。如果不断增大,就会先将hdfs的操作记录写入到一个新文件中,利用secondary namenode来将两者合拼成新的文件,然后再将这个文件的发回NameNode,重新命名。</li></ul><h2 id="写数据过程"><a href="#写数据过程" class="headerlink" title="写数据过程"></a>写数据过程</h2><p>先是客户端请求写入数据,将数据切成每个为128m的数据块,向NameNode请求,拷贝在3个地方,然后NameNode 就会寻找3个需要存放数据的地方,储存成功,向客户端回执,并将数据地址按照升序的方式发送给客户端,客户端开始写数据,将数据和列表发送给第一个DataNode(最近的),然后再第一个DataNode在接收的时候,同时向第二个发送数据和列表,第三个同样如此,完成之后,DataNode会向NameNode回执,表示发送完毕,所以数据发送完毕后,客户端就会请求关闭文件,NameNode会把元数据信息存储到硬盘上。</p><h2 id="读数据过程"><a href="#读数据过程" class="headerlink" title="读数据过程"></a>读数据过程</h2><p>客户端向NameNode请求读取数据(通过文件名),然后NameNode向客户端发送所有数据块的列表信息和每个块的DataNode的列表,然后客户端就会向DataNode发送请求下载数据文件。</p>]]></content>
      
      
      <categories>
          
          <category> 大数据 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Hadoop </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Spring AOP</title>
      <link href="/2019/08/07/spring-aop/"/>
      <url>/2019/08/07/spring-aop/</url>
      
        <content type="html"><![CDATA[]]></content>
      
      
      <categories>
          
          <category> Java </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Java </tag>
            
            <tag> Spring </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Spring IOC</title>
      <link href="/2019/08/07/spring-ioc/"/>
      <url>/2019/08/07/spring-ioc/</url>
      
        <content type="html"><![CDATA[]]></content>
      
      
      <categories>
          
          <category> Java </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Java </tag>
            
            <tag> Spring </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>大数据的学习方向</title>
      <link href="/2019/08/07/da-shu-ju-de-xue-xi-fang-xiang/"/>
      <url>/2019/08/07/da-shu-ju-de-xue-xi-fang-xiang/</url>
      
        <content type="html"><![CDATA[<h2 id="技能"><a href="#技能" class="headerlink" title="技能"></a>技能</h2><pre class=" language-yarn"><code class="language-yarn">基本开发语言 Java Scala PythonSQL能力结构化数据处理流式数据处理ETLKafka统计学分布式数据库</code></pre>]]></content>
      
      
      <categories>
          
          <category> 大数据 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Hadoop </tag>
            
            <tag> Spark </tag>
            
            <tag> Scala </tag>
            
            <tag> Apache </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Spark的优化配置</title>
      <link href="/2019/08/07/spark-de-you-hua-pei-zhi/"/>
      <url>/2019/08/07/spark-de-you-hua-pei-zhi/</url>
      
        <content type="html"><![CDATA[<h2 id="开发调优"><a href="#开发调优" class="headerlink" title="开发调优"></a>开发调优</h2><ol><li>避免创建重复的RDD</li><li>尽可能复用同一个RDD</li><li>对多次使用的RDD持久化</li><li>尽量避免使用shuffle类算子(去重,排序,重分区,聚合,集合或者表操作)</li><li>join前 使用 map-side 预聚合的shuffle操作</li><li>数据压缩</li><li>合并小文件</li></ol><h2 id="资源调优"><a href="#资源调优" class="headerlink" title="资源调优"></a>资源调优</h2><ol><li>num-executors</li><li>executor-memory</li><li>executor-cores</li><li>driver-memory</li><li>spark.default.parallelism: 参数说明∶该参数用于设置每个stage的默认task数量。这个参数极为重要，如果不设置可能会直接影响你的Spark作业性能。</li><li>spark.storage.memoryFraction: 参数说明︰该参数用于设置RDD持久化数据在Executor内存中能占的比例，默认是0.6。也就是说，默认Executor 60%的内存，可以用来保存持久化的RDD数据。根据你选择的不同的持久化策略，如果内存不够时，可能数据就不会持久化，或者数据会写入磁盘。</li></ol><h2 id="数据倾斜调优"><a href="#数据倾斜调优" class="headerlink" title="数据倾斜调优"></a>数据倾斜调优</h2><ol><li>使用Hive ETL预处理数据</li><li>过滤少数导致倾斜的key</li><li>提高shuffle操作的并行度</li><li>两阶段聚合（局部聚合+全局聚合）</li><li>将reduce join转为map join</li><li>采样倾斜key并分拆join操作</li><li>使用随机前缀和扩容RDD进行join</li><li>SQL调优</li></ol><h2 id="Shuffle相关参数"><a href="#Shuffle相关参数" class="headerlink" title="Shuffle相关参数"></a>Shuffle相关参数</h2><ol><li><p>spark.shuffle.file.buffer: </p><ul><li>默认值:32k</li><li>参数说明∶该参数用于设置shuffle write task的BufferedOutputStream的buffer缓冲大小。将数据写到磁盘文件之前，会先写入buffer缓冲中</li><li>数，也就可以减少磁盘I0次数，进而提升性能。在实践中发现，合理调节该参数，性能会有1%~5%的提升。</li></ul></li><li><p>spark.reducer.maxSizelnFlight:</p><ul><li>默认值:48m</li><li>参数说明∶该参数用于设置shuffle read task的buffer缓冲大小，而这个buffer缓冲决定了每次能够拉取多少数据。</li><li>调优建议︰如果作业可用的内存资源较为充足的话，可以适当增加这个参数的大小(比如96m )，从而减少拉取数据的次数，也就可以减少网络传输的次数，进而提升性能。在实践中发现，合理调节该参数，性能会有1%~5%的提升。</li></ul></li><li><p>spark.shuffle.io.maxRetries:</p><ul><li>默认值:3</li><li>参数说明:shuffle read task从shuffle write task所在节点拉取属于自己的数据时，如果因为网络异常导致拉取失败，是会自动进行重试的。该参数就代表了可以重试的最大次数。如果在指定次数之内拉取还是没有成功，就可能会导致作业执行失败。</li><li>调优建议∶对于那些包含了特别耗时的shuffle操作的作业，建议增加重试最大次数（比如60次），以避免由于JVM的full gc或者网络不稳定等因素导致的数据拉取失败。在实践中发现，对于针对超大数据量（数十亿~上百亿）的shuffle过程，调节该参数可以大幅度提升稳定性。</li></ul></li><li><p>spark.shuffle.io.retryWait:</p><ul><li>默认值:5s</li><li>参数说明:具体解释同上，该参数代表了每次重试拉取数据的等待间隔，默认是5s。</li><li>调优建议︰建议加大间隔时长（比如60s )，以增加shuffle操作的稳定性。</li></ul></li><li><p>spark.shuffle.memoryFraction:</p><ul><li>默认值:0.2</li><li>参数说明∶该参数代表了Executor内存中，分配给shuffle read task进行聚合操作的内存比例，默认是20%。</li><li>调优建议︰在资源参数调优中讲解过这个参数。如果内存充足，而且很少使用持久化操作，建议调高这个比例，给shuffle read的聚合操作更多内存，以避免由于内存不足导致聚合过程中频繁读写磁盘。在实践中发现，合理调节该参数可以将性能提升10%左右</li></ul></li><li><p>spark.shuffle.manager:</p><ul><li>默认值:sort</li><li>参数说明∶该参数用于设置ShuffleManager的类型。Spark 1.5以后，有三个可选项:hash、sort和tungsten-sort。HashShuffeManager是Spark1.2以前的默认选项，但是Spark 1.2以及之后的版本默认都是SortShuffleManager了，tungsten-sort与sort类似，但是使用了tungsten计划中的堆外内存管理机制，内存使用效率更高。</li><li>调优建议∶由于SortShuffleManager默认会对数据进行排序，因此如果你的业务逻辑中需要该排序机制的话，则使用默认的SortShuffleManager就可以;而如果你的业务逻辑不需要对数据进行排序，那么建议参考后面的几个参数调优，通过bypass机制或优化的HashShuffeManager来避免排序操作，同时提供较好的磁盘读写性能。这里要注意的是，tungsten-sort要慎用，因为之前发现了一些相应的bug</li></ul></li><li><p>spark.shuffle.sort.bypassMergeThreshold:</p><ul><li>默认值:200</li><li>参数说明∶当ShuffleManager为SortShuffleManager时，如果shuffle read task的数量小于这个阈值（默认是200），则shuffle write过程中不会进行排序操作，而是直接按照未经优化的HashShuffeManager的方式去写数据，但是最后会将每个task产生的所有临时磁盘文件都合并成—个文件，并会创建单独的索引文件。</li><li>调优建议︰当你使用SortShuffleManager时，如果的确不需要排序操作，那么建议将这个参数调大一些，大于shuffle read task的数量。那么此时就会自动启用bypass机制，map-side就不会进行排序了，减少了排序的性能开销。但是这种方式下，依然会产生大量的磁盘文件，因此shuffle write性能有待提高。</li></ul></li><li><p>spark.shuffle.consolidateFiles:</p><ul><li>默认值:false</li><li>参数说明∶如果使用HashShuffleManager，该参数有效。如果设置为true，那么就会开启consolidate机制，会大幅度合并shuffle write的输出文件，对于shuffle read task数量特别多的情况下，这种方法可以极大地减少磁盘IO开销，提升性能。</li><li>调优建议︰如果的确不需要SortShuffleManager的排序机制，那么除了使用bypass机制，还可以尝试将spark.shffle.manager参数手动指定为hash，使用HashShuffleManager，同时开启consolidate机制。在实践中尝试过，发现其性能比开启了bypass机制的SortShuffleManager要高出10%~30%。</li></ul></li></ol>]]></content>
      
      
      <categories>
          
          <category> 大数据 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Spark </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Spark数据库的读取与写入</title>
      <link href="/2019/08/07/spark-shu-ju-ku-de-du-qu-yu-xie-ru/"/>
      <url>/2019/08/07/spark-shu-ju-ku-de-du-qu-yu-xie-ru/</url>
      
        <content type="html"><![CDATA[<h2 id="数据库的读取"><a href="#数据库的读取" class="headerlink" title="数据库的读取"></a>数据库的读取</h2><pre class=" language-scala"><code class="language-scala"><span class="token keyword">def</span> getDataFrame<span class="token punctuation">(</span>tableName<span class="token operator">:</span> <span class="token builtin">String</span><span class="token punctuation">)</span><span class="token operator">:</span> DataFrame <span class="token operator">=</span> <span class="token punctuation">{</span>    <span class="token keyword">var</span> _spark<span class="token operator">:</span> SparkSession <span class="token operator">=</span> _    <span class="token keyword">val</span> spark<span class="token operator">=</span> _spark        <span class="token keyword">var</span> df<span class="token operator">=</span>spark<span class="token punctuation">.</span>read                <span class="token punctuation">.</span>format<span class="token punctuation">(</span><span class="token string">"jdbc"</span><span class="token punctuation">)</span>                <span class="token punctuation">.</span>option<span class="token punctuation">(</span><span class="token string">"driver"</span><span class="token punctuation">,</span> <span class="token string">"com.mysql.cj.jdbc.Driver"</span><span class="token punctuation">)</span>                <span class="token punctuation">.</span>option<span class="token punctuation">(</span><span class="token string">"url"</span><span class="token punctuation">,</span> host<span class="token punctuation">)</span>                <span class="token punctuation">.</span>option<span class="token punctuation">(</span><span class="token string">"dbtable"</span><span class="token punctuation">,</span> tableName<span class="token punctuation">)</span>                <span class="token punctuation">.</span>option<span class="token punctuation">(</span><span class="token string">"user"</span><span class="token punctuation">,</span> user<span class="token punctuation">)</span>                <span class="token punctuation">.</span>option<span class="token punctuation">(</span><span class="token string">"password"</span><span class="token punctuation">,</span> pwd<span class="token punctuation">)</span>                <span class="token punctuation">.</span>load<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">}</span></code></pre><hr><h2 id="数据库的写入"><a href="#数据库的写入" class="headerlink" title="数据库的写入"></a>数据库的写入</h2><pre class=" language-scala"><code class="language-scala"><span class="token keyword">def</span> output<span class="token punctuation">(</span>df<span class="token operator">:</span> DataFrame<span class="token punctuation">,</span> tableName<span class="token operator">:</span> <span class="token builtin">String</span><span class="token punctuation">,</span> mode<span class="token operator">:</span> SaveMode<span class="token punctuation">)</span><span class="token operator">:</span> <span class="token builtin">Unit</span> <span class="token operator">=</span> <span class="token punctuation">{</span>        df<span class="token punctuation">.</span>write                <span class="token punctuation">.</span>format<span class="token punctuation">(</span><span class="token string">"jdbc"</span><span class="token punctuation">)</span>                <span class="token punctuation">.</span>option<span class="token punctuation">(</span><span class="token string">"driver"</span><span class="token punctuation">,</span> <span class="token string">"com.mysql.cj.jdbc.Driver"</span><span class="token punctuation">)</span>                <span class="token punctuation">.</span>option<span class="token punctuation">(</span><span class="token string">"url"</span><span class="token punctuation">,</span> SparkConfig<span class="token punctuation">.</span>host<span class="token punctuation">)</span>                <span class="token punctuation">.</span>option<span class="token punctuation">(</span><span class="token string">"dbtable"</span><span class="token punctuation">,</span> tableName<span class="token punctuation">)</span>                <span class="token punctuation">.</span>option<span class="token punctuation">(</span><span class="token string">"user"</span><span class="token punctuation">,</span> SparkConfig<span class="token punctuation">.</span>user<span class="token punctuation">)</span>                <span class="token punctuation">.</span>option<span class="token punctuation">(</span><span class="token string">"password"</span><span class="token punctuation">,</span> SparkConfig<span class="token punctuation">.</span>pwd<span class="token punctuation">)</span>                <span class="token punctuation">.</span>option<span class="token punctuation">(</span><span class="token string">"batchsize"</span><span class="token punctuation">,</span><span class="token number">50000</span><span class="token punctuation">)</span>                <span class="token punctuation">.</span>mode<span class="token punctuation">(</span>mode<span class="token punctuation">)</span>                <span class="token punctuation">.</span>save<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">}</span></code></pre>]]></content>
      
      
      <categories>
          
          <category> 大数据 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Spark </tag>
            
            <tag> Scala </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Java的学习笔记</title>
      <link href="/2019/08/07/java-de-xue-xi-bi-ji/"/>
      <url>/2019/08/07/java-de-xue-xi-bi-ji/</url>
      
        <content type="html"><![CDATA[<h2 id="学习书籍"><a href="#学习书籍" class="headerlink" title="学习书籍"></a>学习书籍</h2><p>// TODO</p><h2 id="项目"><a href="#项目" class="headerlink" title="项目"></a>项目</h2><p>// TODO</p><h2 id="知识栈"><a href="#知识栈" class="headerlink" title="知识栈"></a>知识栈</h2><p>// TODO</p><h2 id="习惯与思考"><a href="#习惯与思考" class="headerlink" title="习惯与思考"></a>习惯与思考</h2><p>// TODO</p>]]></content>
      
      
      <categories>
          
          <category> Java </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Java </tag>
            
        </tags>
      
    </entry>
    
    
  
  
</search>
